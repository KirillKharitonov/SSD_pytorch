{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Union\nimport torchvision \nfrom torch.optim import lr_scheduler\nfrom torchvision import transforms, datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-18T19:02:50.712413Z","iopub.execute_input":"2022-05-18T19:02:50.712758Z","iopub.status.idle":"2022-05-18T19:02:52.602629Z","shell.execute_reply.started":"2022-05-18T19:02:50.712653Z","shell.execute_reply":"2022-05-18T19:02:52.601871Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:02:52.604351Z","iopub.execute_input":"2022-05-18T19:02:52.604587Z","iopub.status.idle":"2022-05-18T19:02:52.609740Z","shell.execute_reply.started":"2022-05-18T19:02:52.604553Z","shell.execute_reply":"2022-05-18T19:02:52.608997Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import shutil","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:02:52.610899Z","iopub.execute_input":"2022-05-18T19:02:52.611340Z","iopub.status.idle":"2022-05-18T19:02:52.619294Z","shell.execute_reply.started":"2022-05-18T19:02:52.611294Z","shell.execute_reply":"2022-05-18T19:02:52.618545Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((300, 300)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((300, 300)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:02:52.621404Z","iopub.execute_input":"2022-05-18T19:02:52.622028Z","iopub.status.idle":"2022-05-18T19:02:52.629156Z","shell.execute_reply.started":"2022-05-18T19:02:52.621918Z","shell.execute_reply":"2022-05-18T19:02:52.628365Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(range(50)):\n    shutil.copytree('../input/tiny-imagenet/tiny-imagenet-200/train/' + os.listdir('../input/tiny-imagenet/tiny-imagenet-200/train')[i], '/kaggle/working/train/'+os.listdir('../input/tiny-imagenet/tiny-imagenet-200/train')[i])","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:02:52.630571Z","iopub.execute_input":"2022-05-18T19:02:52.630912Z","iopub.status.idle":"2022-05-18T19:05:19.560137Z","shell.execute_reply.started":"2022-05-18T19:02:52.630878Z","shell.execute_reply":"2022-05-18T19:05:19.559458Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(range(50)):\n#     if '../input/tiny-imagenet/tiny-imagenet-200/val/' + os.listdir('../input/tiny-imagenet/tiny-imagenet-200/val')[i] != 'val_annotations.txt':\n    try:\n        shutil.copytree('../input/tiny-imagenet/tiny-imagenet-200/val/' + os.listdir('../input/tiny-imagenet/tiny-imagenet-200/val')[i], '/kaggle/working/val/'+os.listdir('../input/tiny-imagenet/tiny-imagenet-200/val')[i])\n    except:\n        continue","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:06:20.765295Z","iopub.execute_input":"2022-05-18T19:06:20.765555Z","iopub.status.idle":"2022-05-18T19:06:20.799267Z","shell.execute_reply.started":"2022-05-18T19:06:20.765527Z","shell.execute_reply":"2022-05-18T19:06:20.798420Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data_dir = '/kaggle/working/train/'\ntrain_dataset = datasets.ImageFolder(train_data_dir, data_transforms['train'])\n\n# val_data_dir = '/kaggle/working/val/'\n# val_dataset = datasets.ImageFolder(val_data_dir, data_transforms['val'])","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:07:07.820649Z","iopub.execute_input":"2022-05-18T19:07:07.821351Z","iopub.status.idle":"2022-05-18T19:07:07.963261Z","shell.execute_reply.started":"2022-05-18T19:07:07.821318Z","shell.execute_reply":"2022-05-18T19:07:07.962543Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_loaders = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:07:38.148568Z","iopub.execute_input":"2022-05-18T19:07:38.149372Z","iopub.status.idle":"2022-05-18T19:07:38.155081Z","shell.execute_reply.started":"2022-05-18T19:07:38.149335Z","shell.execute_reply":"2022-05-18T19:07:38.153235Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class_names = train_dataset.classes\n","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:07:40.099809Z","iopub.execute_input":"2022-05-18T19:07:40.100348Z","iopub.status.idle":"2022-05-18T19:07:40.103752Z","shell.execute_reply.started":"2022-05-18T19:07:40.100311Z","shell.execute_reply":"2022-05-18T19:07:40.102867Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class_names","metadata":{"execution":{"iopub.status.busy":"2022-05-18T19:07:43.538587Z","iopub.execute_input":"2022-05-18T19:07:43.539255Z","iopub.status.idle":"2022-05-18T19:07:43.546007Z","shell.execute_reply.started":"2022-05-18T19:07:43.539218Z","shell.execute_reply":"2022-05-18T19:07:43.545213Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"nclasses = len(os.listdir(train_data_dir))\nnclasses","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:25:33.877216Z","iopub.execute_input":"2022-04-25T06:25:33.878089Z","iopub.status.idle":"2022-04-25T06:25:33.89242Z","shell.execute_reply.started":"2022-04-25T06:25:33.878027Z","shell.execute_reply":"2022-04-25T06:25:33.89136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.figure(figsize=(15, 12))\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)\n\n\n# Получим 1 батч (картнки-метки) из обучающей выборки\ninputs, classes = next(iter(train_loaders['train']))\n\n# Расположим картинки рядом\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:25:51.026575Z","iopub.execute_input":"2022-04-25T06:25:51.026931Z","iopub.status.idle":"2022-04-25T06:25:51.811156Z","shell.execute_reply.started":"2022-04-25T06:25:51.026877Z","shell.execute_reply":"2022-04-25T06:25:51.810377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = [\n    # expand ratio, channels, layers, kernel_size, stride\n    [1, 16, 1, 3, 1],\n    [6, 24, 2, 3, 2],\n    [6, 40, 2, 5, 2],\n    [6, 80, 3, 5, 1],\n    [6, 112, 3, 5, 1],\n    [6, 192, 4, 5, 2],\n    [6, 320, 1, 3, 1]\n]\n\nphi_vals = {\n    # model_version : (phi_value, resolution, drop_rate)\n    'e0': (0, 224, 0.2),\n    'e1': (0.5, 240, 0.2),\n    'e2': (1, 260, 0.3),\n    'e3': (2, 300, 0.3),\n    'e4': (3, 380, 0.4),\n    'e5': (4, 456, 0.4),\n    'e6': (5, 528, 0.5),\n    'e7': (6, 600, 0.5),\n}\n\nstem_params = [3, 32, 2, 1]\n\n\nclass ConvLayer(nn.Module):\n    \"\"\"\n    Layer for convolution operations. It aggregates also batch normalization and activation functions\n    \"\"\"\n\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            padding: int = 1,\n            groups: int = 1,\n            bn: bool = True,\n            act: bool = True\n    ):\n        super(ConvLayer, self).__init__()\n\n        self.conv = nn.Conv2d(\n            in_channels = in_channels,\n            out_channels = out_channels,\n            kernel_size = kernel_size,\n            stride = stride,\n            padding = padding,\n            groups = groups\n        )\n        self.batch_norm = nn.BatchNorm2d(out_channels) if bn else nn.Identity()\n        self.activation = nn.SiLU() if act else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.conv(x)\n        x = self.batch_norm(x)\n        x = self.activation(x)\n        return x\n\n\ndef drop_connect(\n        inputs: torch.Tensor,\n        p: float = 0.1,\n        training: bool = True\n) -> torch.Tensor:\n    \"\"\"\n    drop c\n    :param inputs:\n    :param p:\n    :param training:\n    :return:\n    \"\"\"\n    assert 0 <= p <= 1, 'p must be in range of [0, 1]'\n    if not training:\n        return inputs\n\n    keep_prob = 1 - p\n    random_tensor = keep_prob\n    random_tensor += torch.rand([inputs.shape[0], 1, 1, 1], dtype = inputs.dtype, device = inputs.device)\n    binary_tensor = torch.floor(random_tensor)\n\n    output = inputs / keep_prob * binary_tensor\n    return output\n\n\nclass SEmodule(nn.Module):\n    \"\"\"\n    Interdependencies between the channels of convolutional features\n    https://arxiv.org/pdf/1709.01507.pdf\n    \"\"\"\n\n    def __init__(\n            self,\n            in_channels: int,\n            scale_factor: int = 24\n    ):\n        super(SEmodule, self).__init__()\n\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_1 = nn.Conv2d(\n            in_channels = in_channels,\n            out_channels = in_channels // scale_factor,\n            kernel_size = 1,\n            stride = 1,\n        )\n\n        self.activation_1 = nn.SiLU()\n        self.conv_2 = nn.Conv2d(\n            in_channels = in_channels // scale_factor,\n            out_channels = in_channels,\n            kernel_size = 1,\n            stride = 1,\n        )\n        self.activation_2 = nn.Sigmoid()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        y = self.pool(x)\n        y = self.activation_1(self.conv_1(y))\n        y = self.activation_2(self.conv_2(y))\n\n        return x * y\n\n\n#\nclass MBConvBlock(nn.Module):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            expansion_f: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            scale_f: int = 24,\n            training: bool = True,\n            drop_p: float = 0.2\n    ):\n        super(MBConvBlock, self).__init__()\n\n        expanded_ch = in_channels * expansion_f\n\n        padding = int(np.ceil((kernel_size - stride) / 2))\n\n        self.skip_connection = (in_channels == out_channels) and (stride == 1)\n\n        self.expansion = ConvLayer(\n            in_channels,\n            expanded_ch,\n            kernel_size = 3,\n            stride = 1,\n            padding = 1\n        )\n\n        self.depthwise = ConvLayer(\n            expanded_ch, expanded_ch,\n            kernel_size, stride,\n            padding, expanded_ch\n        )\n\n        self.se = SEmodule(expanded_ch, scale_factor = scale_f)\n\n        self.reducer = ConvLayer(\n            expanded_ch, out_channels,\n            padding = 0,\n            kernel_size = 1, act = False\n        )\n\n        self.training = training\n        self.drop_p = drop_p\n\n    def forward(\n            self,\n            x: torch.Tensor\n    ) -> torch.Tensor:\n        input = x\n\n        x = self.expansion(x)\n        x = self.depthwise(x)\n        x = self.se(x)\n        x = self.reducer(x)\n        if self.skip_connection:\n            x = drop_connect(x, self.drop_p)\n            x += input\n\n        return x\n\n\nclass MBConv1(MBConvBlock):\n\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            scale_f: int = 24,\n            training: bool = True,\n            drop_p: float = 0\n    ):\n        super(MBConv1, self).__init__(\n            in_channels = in_channels,\n            out_channels = out_channels,\n            expansion_f = 1,\n            kernel_size = kernel_size,\n            stride = stride,\n            scale_f = scale_f,\n            training = training,\n            drop_p = 0.1\n        )\n\n\nclass MBConv6(MBConvBlock):\n\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            scale_f: int = 24,\n            training: bool = True,\n            drop_p: float = 0\n    ):\n        super(MBConv6, self).__init__(\n            in_channels = in_channels,\n            out_channels = out_channels,\n            expansion_f = 6,\n            kernel_size = kernel_size,\n            stride = stride,\n            scale_f = scale_f,\n            training = training,\n            drop_p = 0.1\n        )\n\n\n# def scale_width(w, w_factor):\n#     \"\"\"Scales width given a scale factor\"\"\"\n#     w *= w_factor\n#     new_w = (int(w + 4) // 8) * 8\n#     new_w = max(8, new_w)\n#     if new_w < 0.9 * w:\n#         new_w += 8\n#     return int(new_w)\n\n\nclass EfficientNet(nn.Module):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self,\n                 version: str,\n                 num_classes: int,\n                 last_channels: int = 1280\n                 # mb1_params: list,\n                 # mb6_params: dict,\n                 # last_conv_params: list,\n                 # stem_params: list = [3, 32, 2, 1],\n                 # num_of_mb6_blocks: int = 6,\n                 # out_size: int = 1000,\n                 # w_factor: Union[int, float] = 1,\n                 # d_factor: Union[int, float] = 1,\n\n                 ):\n        super(EfficientNet, self).__init__()\n\n        width_factor, depth_factor, drop_rate = self.calculate_factors(version)\n        self.drop_rate = drop_rate\n        self.last_channels = int(np.ceil(last_channels * width_factor))\n        # Stem\n        self.stem_conv = ConvLayer(*stem_params)\n\n        # Features\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.features = self.create_features(width_factor, depth_factor, self.last_channels)\n\n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(last_channels, num_classes)\n        )\n\n        self._initialize_weights()\n\n    def _initialize_weights(self) -> None:\n        \"\"\"\n        Weights initialization.\n        For convolutional blocks there is \"He initialization\".\n        :return:\n            None\n        \"\"\"\n        for module in self.modules():\n            if isinstance(module, nn.Conv2d):\n                nn.init.kaiming_normal_(module.weight)\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n            elif isinstance(module, nn.BatchNorm2d):\n                nn.init.constant_(module.weight, 1)\n\n    def calculate_factors(\n            self,\n            version: str,\n            alpha: float = 1.2,\n            beta: float = 1.1\n    ):\n        phi, res, drop_ = phi_vals[version]\n        depth_factor = alpha ** phi\n        width_factor = beta ** phi\n        return width_factor, depth_factor, drop_\n\n    def create_features(\n            self,\n            width_factor: float,\n            depth_factor: float,\n            last_channels: int\n    ):\n\n        in_channels = int(stem_params[1] * width_factor)\n        features = []\n\n        curr_block = 0\n        for expand_ratio, channels, layers, kernel_size, stride in base_model:\n            out_channels = int(4 * np.ceil((channels * width_factor) / 4))\n            scaled_layers = int(np.ceil(layers * depth_factor))\n\n            for layer in range(scaled_layers):\n                if curr_block == 0:\n                    features.append(\n                        MBConv1(\n                            in_channels,\n                            out_channels,\n                            kernel_size,\n                            stride = stride if layer == 0 else 1,\n                            drop_p = self.drop_rate\n                        )\n                    )\n\n                    in_channels = out_channels\n                else:\n                    features.append(\n                        MBConv6(\n                            in_channels,\n                            out_channels,\n                            kernel_size,\n                            stride = stride if layer == 0 else 1,\n                            drop_p = self.drop_rate\n                        )\n                    )\n                    in_channels = out_channels\n\n                curr_block += 1\n\n            features.append(\n                ConvLayer(\n                    in_channels,\n                    last_channels,\n                    kernel_size = 1,\n                    stride = 1,\n                    padding = 0\n                )\n            )\n\n            return nn.Sequential(*features)\n\n    def forward(\n            self,\n            x: torch.Tensor\n    ) -> torch.Tensor:\n\n        x = self.stem_conv(x)\n        x = self.pool(self.features(x))\n        x = self.classifier(x.view(x.shape[0], -1))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:25:54.478007Z","iopub.execute_input":"2022-04-25T06:25:54.478299Z","iopub.status.idle":"2022-04-25T06:25:54.534754Z","shell.execute_reply.started":"2022-04-25T06:25:54.478261Z","shell.execute_reply":"2022-04-25T06:25:54.533618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:25:54.857404Z","iopub.execute_input":"2022-04-25T06:25:54.857669Z","iopub.status.idle":"2022-04-25T06:25:54.928695Z","shell.execute_reply.started":"2022-04-25T06:25:54.85764Z","shell.execute_reply":"2022-04-25T06:25:54.927558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = EfficientNet('e0', nclasses).to(device)\nloss = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                                  lr=1e-3)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:25:55.156892Z","iopub.execute_input":"2022-04-25T06:25:55.157828Z","iopub.status.idle":"2022-04-25T06:25:58.658241Z","shell.execute_reply.started":"2022-04-25T06:25:55.157783Z","shell.execute_reply":"2022-04-25T06:25:58.657225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.autonotebook import tqdm, trange\nimport time","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:25:58.660162Z","iopub.execute_input":"2022-04-25T06:25:58.660465Z","iopub.status.idle":"2022-04-25T06:25:58.666281Z","shell.execute_reply.started":"2022-04-25T06:25:58.660422Z","shell.execute_reply":"2022-04-25T06:25:58.665014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=35):\n    since = time.time()\n\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n    \n    #Ваш код здесь\n    losses = {'train': [], \"valid\": []}\n\n    pbar = trange(num_epochs, desc=\"Epoch:\")\n\n    for epoch in pbar:\n\n        # каждя эпоха имеет обучающую и тестовую стадии\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                scheduler.step()\n                model.train(True)  # установаить модель в режим обучения\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # итерируемся по батчам\n            for data in tqdm(train_loaders[phase], leave=False, desc=f\"{phase} iter:\"):\n                # получаем картинки и метки\n                inputs, labels = data\n\n                # оборачиваем в переменные\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n\n                # инициализируем градиенты параметров\n                if phase==\"train\":\n                    optimizer.zero_grad()\n\n                # forward pass\n                if phase == \"eval\":\n                    with torch.no_grad():\n                        outputs = model(inputs)\n                else:\n                    outputs = model(inputs)\n                preds = torch.argmax(outputs, -1)\n                loss = criterion(outputs, labels)\n\n                # backward pass + оптимизируем только если это стадия обучения\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n                # статистика\n                running_loss += loss.item()\n                running_corrects += int(torch.sum(preds == labels.data))\n\n            epoch_loss = running_loss / len(train_loaders[phase])\n            epoch_acc = running_corrects / len(train_loaders[phase])\n            \n            # Ваш код здесь\n            losses[phase].append(epoch_loss)\n            \n            pbar.set_description('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                                    phase, epoch_loss, epoch_acc\n                                ))\n\n            # если достиглось лучшее качество, то запомним веса модели\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = model.state_dict()\n                torch.save(model_extractor.state_dict(), '/kaggle/working/efficientnet_v0.pt')\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # загрузим лучшие веса модели\n    model.load_state_dict(best_model_wts)\n    return model, losses","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:25:58.668311Z","iopub.execute_input":"2022-04-25T06:25:58.66892Z","iopub.status.idle":"2022-04-25T06:25:58.688894Z","shell.execute_reply.started":"2022-04-25T06:25:58.668877Z","shell.execute_reply":"2022-04-25T06:25:58.687803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_extractor, losses = train_model(model, loss, optimizer, scheduler)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:25:58.691377Z","iopub.execute_input":"2022-04-25T06:25:58.691799Z","iopub.status.idle":"2022-04-25T11:51:02.449437Z","shell.execute_reply.started":"2022-04-25T06:25:58.691753Z","shell.execute_reply":"2022-04-25T11:51:02.447435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2022-04-22T06:49:04.097616Z","iopub.execute_input":"2022-04-22T06:49:04.097913Z","iopub.status.idle":"2022-04-22T06:49:05.457923Z","shell.execute_reply.started":"2022-04-22T06:49:04.097871Z","shell.execute_reply":"2022-04-22T06:49:05.457184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model_extractor.state_dict(), './efficientnet_v0.pt')","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:51:07.62049Z","iopub.execute_input":"2022-04-25T11:51:07.620801Z","iopub.status.idle":"2022-04-25T11:51:07.641195Z","shell.execute_reply.started":"2022-04-25T11:51:07.62077Z","shell.execute_reply":"2022-04-25T11:51:07.640006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:41:12.869794Z","iopub.execute_input":"2022-04-24T19:41:12.870566Z","iopub.status.idle":"2022-04-24T19:41:12.874006Z","shell.execute_reply.started":"2022-04-24T19:41:12.870524Z","shell.execute_reply":"2022-04-24T19:41:12.873329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:41:22.508909Z","iopub.execute_input":"2022-04-24T19:41:22.509581Z","iopub.status.idle":"2022-04-24T19:41:22.51407Z","shell.execute_reply.started":"2022-04-24T19:41:22.509546Z","shell.execute_reply":"2022-04-24T19:41:22.513408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}