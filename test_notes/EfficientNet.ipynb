{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "gAjFG6ALChF0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer for convolution operations. It aggregates also batch normalization and activation functions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            kernel_size: int = 3,\n",
        "            stride: int = 1,\n",
        "            padding: int = 1,\n",
        "            groups: int = 1,\n",
        "            bn: bool = True,\n",
        "            act: bool = True\n",
        "    ):\n",
        "        super(ConvLayer, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = out_channels,\n",
        "            kernel_size = kernel_size,\n",
        "            stride = stride,\n",
        "            padding = padding\n",
        "        )\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels) if bn else nn.Identity()\n",
        "        self.activation = nn.SiLU() if act else nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def drop_connect(\n",
        "        inputs: torch.Tensor,\n",
        "        p: float = 0.1,\n",
        "        training: bool = True\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    drop c\n",
        "    :param inputs:\n",
        "    :param p:\n",
        "    :param training:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    assert 0 <= p <= 1, 'p must be in range of [0, 1]'\n",
        "    if not training:\n",
        "        return inputs\n",
        "\n",
        "    keep_prob = 1 - p\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += torch.rand([inputs.shape[0], 1, 1, 1], dtype = inputs.dtype, device = inputs.device)\n",
        "    binary_tensor = torch.floor(random_tensor)\n",
        "\n",
        "    output = inputs / keep_prob * binary_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class SEmodule(nn.Module):\n",
        "    \"\"\"\n",
        "    Interdependencies between the channels of convolutional features\n",
        "    https://arxiv.org/pdf/1709.01507.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            scale_factor: int = 24\n",
        "    ):\n",
        "        super(SEmodule, self).__init__()\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv_1 = nn.Conv2d(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = in_channels // scale_factor,\n",
        "            kernel_size = 1,\n",
        "            stride = 1,\n",
        "        )\n",
        "\n",
        "        self.activation_1 = nn.SiLU()\n",
        "        self.conv_2 = nn.Conv2d(\n",
        "            in_channels = in_channels // scale_factor,\n",
        "            out_channels = in_channels,\n",
        "            kernel_size = 1,\n",
        "            stride = 1,\n",
        "        )\n",
        "        self.activation_2 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        y = self.pool(x)\n",
        "        y = self.activation_1(self.conv_1(y))\n",
        "        y = self.activation_2(self.conv_2(y))\n",
        "\n",
        "        return x * y\n",
        "\n",
        "\n",
        "#\n",
        "class MBConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            expansion_f: int,\n",
        "            kernel_size: int = 3,\n",
        "            stride: int = 1,\n",
        "            scale_f: int = 24,\n",
        "            training: bool = True,\n",
        "            drop_p: float = 0.2\n",
        "    ):\n",
        "        super(MBConvBlock, self).__init__()\n",
        "\n",
        "        expanded_ch = in_channels * expansion_f\n",
        "\n",
        "        padding = int(np.ceil((kernel_size - stride) / 2))\n",
        "\n",
        "        self.skip_connection = (in_channels == out_channels) and (stride == 1)\n",
        "\n",
        "        self.expansion = ConvLayer(\n",
        "            in_channels,\n",
        "            expanded_ch,\n",
        "            kernel_size,\n",
        "            stride\n",
        "        )\n",
        "        \n",
        "        self.depthwise = ConvLayer(\n",
        "            expanded_ch, expanded_ch,\n",
        "            kernel_size, stride,\n",
        "            padding, expanded_ch\n",
        "        )\n",
        "\n",
        "        self.se = SEmodule(expanded_ch, scale_factor = scale_f)\n",
        "\n",
        "        self.reducer = ConvLayer(\n",
        "            expanded_ch, out_channels,\n",
        "            padding = 0,\n",
        "            kernel_size = 1, act = False\n",
        "        )\n",
        "\n",
        "        self.training = training\n",
        "        self.drop_p = drop_p\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        input = x\n",
        "\n",
        "        x = self.expansion(x)\n",
        "        x = self.depthwise(x)\n",
        "        x = self.se(x)\n",
        "        x = self.reducer(x)\n",
        "        if self.skip_connection:\n",
        "            x = drop_connect(x, self.drop_p)\n",
        "            x += input\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MBConv1(MBConvBlock):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            kernel_size: int = 3,\n",
        "            stride: int = 1,\n",
        "            scale_f: int = 24,\n",
        "            training: bool = True,\n",
        "            drop_p: float = 0\n",
        "    ):\n",
        "        super(MBConv1, self).__init__(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = out_channels,\n",
        "            expansion_f = 1,\n",
        "            kernel_size = kernel_size,\n",
        "            stride = stride,\n",
        "            scale_f = scale_f,\n",
        "            training = training,\n",
        "            drop_p = 0.1\n",
        "        )\n",
        "\n",
        "class MBConv6(MBConvBlock):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            kernel_size: int = 3,\n",
        "            stride: int = 1,\n",
        "            scale_f: int = 24,\n",
        "            training: bool = True,\n",
        "            drop_p: float = 0\n",
        "    ):\n",
        "        super(MBConv6, self).__init__(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = out_channels,\n",
        "            expansion_f = 6,\n",
        "            kernel_size = kernel_size,\n",
        "            stride = stride,\n",
        "            scale_f = scale_f,\n",
        "            training = training,\n",
        "            drop_p = 0.1\n",
        "        )\n",
        "\n",
        "\n",
        "def scale_width(w, w_factor):\n",
        "  \"\"\"Scales width given a scale factor\"\"\"\n",
        "  w *= w_factor\n",
        "  new_w = (int(w+4) // 8) * 8\n",
        "  new_w = max(8, new_w)\n",
        "  if new_w < 0.9*w:\n",
        "     new_w += 8\n",
        "  return int(new_w)\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                  mb1_params: list,\n",
        "            mb6_params: dict,\n",
        "            last_conv_params: list,\n",
        "            stem_params: list = [3, 32, 2, 1],\n",
        "            num_of_mb6_blocks: int = 6,\n",
        "            out_size: int = 1000,\n",
        "            w_factor: Union[int, float] = 1,\n",
        "            d_factor: Union[int, float] = 1,\n",
        "           \n",
        "    ):\n",
        "        super(EfficientNet, self).__init__()\n",
        "\n",
        "        assert num_of_mb6_blocks == len(mb6_params.keys()), \\\n",
        "            'number of MBConv6 layers must be the same as amount of keys in dict'\n",
        "\n",
        "        # Stem\n",
        "        self.stem_conv = ConvLayer(*stem_params)\n",
        "\n",
        "        # first MBConv1 block\n",
        "        self.mbconv1 = MBConv1(*mb1_params)\n",
        "\n",
        "        # second block\n",
        "        self.mbconv6_blocks = nn.ModuleList([])\n",
        "        for i in range(num_of_mb6_blocks):\n",
        "            self.mbconv6_blocks.append(\n",
        "                MBConv6(*mb6_params[i])\n",
        "            )\n",
        "\n",
        "        self.conv = ConvLayer(*last_conv_params)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(last_conv_params[1], out_size)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "        \n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Weights initialization.\n",
        "        For convolutional blocks there is \"He initialization\".\n",
        "        :return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                nn.init.constant_(module.weight, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.stem_conv(x)\n",
        "        print(x.shape)\n",
        "        x = self.mbconv1(x)\n",
        "        print(x.shape)\n",
        "        for block in self.mbconv6_blocks:\n",
        "            x = block(x)\n",
        "            print(x.shape)\n",
        "        x = self.conv(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((10, 3, 300, 300))"
      ],
      "metadata": {
        "id": "X4AyL0bECnQC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mb1_params = [32, 16, 3, 2, 1]\n",
        "mb6_params = {\n",
        "    0: [16, 24, 3, 1],\n",
        "    1: [24, 40, 3, 2], \n",
        "    2: [40, 80, 5, 2],\n",
        "    3: [80, 112, 3, 2],\n",
        "    4: [112, 192, 5, 1],\n",
        "    5: [192, 320, 5, 1]\n",
        "}\n",
        "last_conv_params = [320, 1280, 3, 1]"
      ],
      "metadata": {
        "id": "379lJNk-RPz0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficient =  EfficientNet(mb1_params, mb6_params, last_conv_params)"
      ],
      "metadata": {
        "id": "2dr-YdlmUL16"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = efficient(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "ckw1C2rwULX1",
        "outputId": "77f94a28-00ce-470d-eccc-5ddb3fcc3053"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 32, 301, 301])\n",
            "torch.Size([10, 16, 76, 76])\n",
            "torch.Size([10, 24, 76, 76])\n",
            "torch.Size([10, 40, 19, 19])\n",
            "torch.Size([10, 80, 5, 5])\n",
            "torch.Size([10, 112, 2, 2])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-cc09fcb77ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mefficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-83e7b456e722>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmbconv6_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-83e7b456e722>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpansion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepthwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-83e7b456e722>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (4 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lWMTeC6pUY3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv = ConvLayer(*mb1_params)"
      ],
      "metadata": {
        "id": "MAS7rpjVRPgo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_c = conv(x)"
      ],
      "metadata": {
        "id": "ATFXS9J2RO-V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_c.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt0hECqKRwHE",
        "outputId": "bea1b644-0453-42c7-c69b-ee9b5246bdb9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 32, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem = [3, 32, 2, 1]\n",
        "conv = nn.Conv2d(*stem)"
      ],
      "metadata": {
        "id": "h5jGa0F8FoAR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mb = MBConvBlock(in_channels= 3, out_channels=3, expansion_f = 24)"
      ],
      "metadata": {
        "id": "X6clye3lD1M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mb_out = mb(x)"
      ],
      "metadata": {
        "id": "mRcfLjcND2XE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef54cca7-1dd7-42d4-c3b2-d95171c99e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([10, 3, 64, 64])\n",
            "shape after expansion: torch.Size([10, 72, 64, 64])\n",
            "shape after depthwise: torch.Size([10, 72, 64, 64])\n",
            "shape after se: torch.Size([10, 72, 64, 64])\n",
            "shape after reducer: torch.Size([10, 3, 64, 64])\n",
            "skipping\n"
          ]
        }
      ]
    }
  ]
}