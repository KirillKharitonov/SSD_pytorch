{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gAjFG6ALChF0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer for convolution operations. It aggregates also batch normalization and activation functions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            kernel_size: int = 3,\n",
        "            stride: int = 1,\n",
        "            padding: int = 1,\n",
        "            groups: int = 1,\n",
        "            bn: bool = True,\n",
        "            act: bool = True\n",
        "    ):\n",
        "        super(ConvLayer, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = out_channels,\n",
        "            kernel_size = kernel_size,\n",
        "            stride = stride,\n",
        "            padding = padding\n",
        "        )\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels) if bn else nn.Identity()\n",
        "        self.activation = nn.SiLU() if act else nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def drop_connect(\n",
        "        inputs: torch.Tensor,\n",
        "        p: float = 0.1,\n",
        "        training: bool = True\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    drop c\n",
        "    :param inputs:\n",
        "    :param p:\n",
        "    :param training:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    assert 0 <= p <= 1, 'p must be in range of [0, 1]'\n",
        "    if not training:\n",
        "        return inputs\n",
        "\n",
        "    keep_prob = 1 - p\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += torch.rand([inputs.shape[0], 1, 1, 1], dtype = inputs.dtype, device = inputs.device)\n",
        "    binary_tensor = torch.floor(random_tensor)\n",
        "\n",
        "    output = inputs / keep_prob * binary_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class SEmodule(nn.Module):\n",
        "    \"\"\"\n",
        "    Interdependencies betwee the channels of convolutional features\n",
        "    https://arxiv.org/pdf/1709.01507.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            scale_factor: int = 24\n",
        "    ):\n",
        "        super(SEmodule, self).__init__()\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv_1 = nn.Conv2d(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = in_channels // scale_factor,\n",
        "            kernel_size = 1,\n",
        "            stride = 1,\n",
        "        )\n",
        "\n",
        "        self.activation_1 = nn.SiLU()\n",
        "        self.conv_2 = nn.Conv2d(\n",
        "            in_channels = in_channels // scale_factor,\n",
        "            out_channels = in_channels,\n",
        "            kernel_size = 1,\n",
        "            stride = 1,\n",
        "        )\n",
        "        self.activation_2 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        y = self.pool(x)\n",
        "        y = self.activation_1(self.conv_1(y))\n",
        "        y = self.activation_2(self.conv_2(y))\n",
        "\n",
        "        return x * y\n",
        "\n",
        "\n",
        "#\n",
        "class MBConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            expansion_f: int,\n",
        "            kernel_size: int = 3,\n",
        "            stride: int = 1,\n",
        "            scale_f: int = 24,\n",
        "            training: bool = True,\n",
        "            drop_p: float = 0.2\n",
        "    ):\n",
        "        super(MBConvBlock, self).__init__()\n",
        "\n",
        "        expanded_ch = in_channels * expansion_f\n",
        "\n",
        "        padding = int(np.ceil((kernel_size - stride) / 2))\n",
        "\n",
        "        self.skip_connection = (in_channels == out_channels) and (stride == 1)\n",
        "\n",
        "        self.expansion = nn.Conv2d(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = expanded_ch,\n",
        "            kernel_size = 1\n",
        "        )\n",
        "\n",
        "        self.depthwise = ConvLayer(\n",
        "            expanded_ch, expanded_ch,\n",
        "            kernel_size, stride,\n",
        "            padding, expanded_ch\n",
        "        )\n",
        "\n",
        "        self.se = SEmodule(expanded_ch, scale_factor = scale_f)\n",
        "\n",
        "        self.reducer = ConvLayer(\n",
        "            expanded_ch, out_channels, \n",
        "            padding = 0,\n",
        "            kernel_size = 1, act = False\n",
        "        )\n",
        "\n",
        "        self.training = training\n",
        "        self.drop_p = drop_p\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "        input = x\n",
        "        print(f'input shape: {input.shape}')\n",
        "\n",
        "        x = self.expansion(x)\n",
        "        print(f'shape after expansion: {x.shape}')\n",
        "\n",
        "        x = self.depthwise(x)\n",
        "        print(f'shape after depthwise: {x.shape}')\n",
        "        x = self.se(x)\n",
        "        print(f'shape after se: {x.shape}')\n",
        "        x = self.reducer(x)\n",
        "        print(f'shape after reducer: {x.shape}')\n",
        "        if self.skip_connection:\n",
        "          print('skipping')\n",
        "          x = drop_connect(x, self.drop_p)\n",
        "          x += input\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((10, 3, 64, 64))"
      ],
      "metadata": {
        "id": "X4AyL0bECnQC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mb = MBConvBlock(in_channels= 3, out_channels=3, expansion_f = 24)"
      ],
      "metadata": {
        "id": "X6clye3lD1M4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mb_out = mb(x)"
      ],
      "metadata": {
        "id": "mRcfLjcND2XE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef54cca7-1dd7-42d4-c3b2-d95171c99e5a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([10, 3, 64, 64])\n",
            "shape after expansion: torch.Size([10, 72, 64, 64])\n",
            "shape after depthwise: torch.Size([10, 72, 64, 64])\n",
            "shape after se: torch.Size([10, 72, 64, 64])\n",
            "shape after reducer: torch.Size([10, 3, 64, 64])\n",
            "skipping\n"
          ]
        }
      ]
    }
  ]
}